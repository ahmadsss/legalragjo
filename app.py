import streamlit as st
import openai
import weaviate
from dotenv import load_dotenv
import os
from weaviate.classes.init import Auth

# Load .env (optional for local dev)
load_dotenv()

# Load secrets from Streamlit Cloud
openai.api_key = st.secrets["OPENAI_API_KEY"]
WEAVIATE_API_KEY = st.secrets["WEAVIATE_API_KEY"]
WEAVIATE_URL = st.secrets["WEAVIATE_URL"]

# Connect to Weaviate Cloud
client = weaviate.connect_to_weaviate_cloud(
    cluster_url=WEAVIATE_URL,
    auth_credentials=Auth.api_key(WEAVIATE_API_KEY),
)

# ğŸ” Embed query using OpenAI
def embed_query(text):
    response = openai.embeddings.create(
        input=text,
        model="text-embedding-3-large"
    )
    return response.data[0].embedding

# ğŸ” Semantic retrieval from Weaviate
def retrieve_articles(query, limit=5):
    vector = embed_query(query)
    results = client.collections.get("LawArticle").query.near_vector(
        near_vector=vector,
        limit=limit
    )
    return results.objects

# ğŸ§  Generate a legal-style answer using GPT-4
def generate_answer(question, context):
    context_text = "\n\n".join(
        f"Ø§Ù„Ù…Ø§Ø¯Ø© {o.properties.get('article_number', '')}: {o.properties.get('article_title', '')}\n{o.properties.get('text', '')}"
        for o in context
    )

    prompt = f"""You are a legal expert and consultant. For every legal question:

    Base your answer only on the content of the retrieved chunks from the vector database. Do not answer from general knowledge or pre-training unless explicitly instructed.

    For each legal point you provide, cite or quote the relevant retrieved chunk (article, clause, or paragraph) that supports your answer.

    Structure your answer as a numbered list of clear legal situations or rights, each point referencing the supporting chunk.

    If multiple chunks address the same issue (e.g., general and specific provisions), present them together, clarifying their relationship.

    Always mention any legal steps required before action (e.g., giving notice, going to court) if stated in the retrieved chunks.

    After the main list, briefly explain why these chunks are relevant to the question, referencing their position (general rule, special rule, etc.).

    End every answer with: "Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙÙ‚Ø· ÙˆÙ„ÙŠØ³Øª Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø±Ø³Ù…ÙŠØ©."

    Use the tone, depth, and legal structure of an expert consultant, aiming for the detail and clarity seen in the best large language model (LLM) responses.

    Never miss a general legal rule from the retrieved chunks that may apply, even if a special rule exists.
    Answer according to the retrieved chunks. Do not rely on your own legal knowledge. Structure your answer by legal points, and cite the chunk or article for every claim. Use professional legal language and reasoning.
 Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:
 
{context_text}

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""
    completion = openai.chat.completions.create(
        model="gpt-4.1",
        messages=[
            {"role": "system", "content": "Ø£Ø¬Ø¨ ÙÙ‚Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø©."},
            {"role": "user", "content": prompt}
        ]
    )
    return completion.choices[0].message.content.strip()

# ğŸŒ Streamlit Web UI
st.set_page_config(layout="centered", page_title="Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø°ÙƒÙŠ")
st.markdown("<h1 style='text-align: right; direction: rtl;'>ğŸ’¼ Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø£Ø±Ø¯Ù†ÙŠ</h1>", unsafe_allow_html=True)

question = st.text_input(
    "âœï¸ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù‡Ù†Ø§:",
    key="query",
    placeholder="Ù…Ø§ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø°ÙŠ Ø¬Ø±Ù‰ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø§Ø¯Ø© 8ØŸ",
    help="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
)

if question:
    with st.spinner("ğŸ” ÙŠØªÙ… Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©..."):
        articles = retrieve_articles(question)

    if not articles:
        st.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙˆØ§Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.")
    else:
        with st.spinner("ğŸ¤– ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
            answer = generate_answer(question, articles)

        st.markdown("### ğŸ§  Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©", unsafe_allow_html=True)
        # RTL-formatted answer, preserving paragraph breaks and Arabic quoting
        st.markdown(
            f"""
            <div style='direction: rtl; text-align: right; font-size: 1.15em; line-height: 2.1;'>
            {answer.replace(chr(10), '<br>')}
            </div>
            """,
            unsafe_allow_html=True
        )

        with st.expander("ğŸ“œ Ø¹Ø±Ø¶ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©"):
            for obj in articles:
                st.markdown(
                    f"<div style='direction: rtl; text-align: right;'><b>Ø§Ù„Ù…Ø§Ø¯Ø© {obj.properties.get('article_number')}</b> - {obj.properties.get('article_title')}</div>",
                    unsafe_allow_html=True
                )
                st.markdown(
                    f"<div style='direction: rtl; text-align: right; background-color: #f8f9fa; border-radius: 8px; padding: 8px; margin-bottom: 10px;'>{obj.properties.get('text').replace(chr(10), '<br>')}</div>",
                    unsafe_allow_html=True
                )
